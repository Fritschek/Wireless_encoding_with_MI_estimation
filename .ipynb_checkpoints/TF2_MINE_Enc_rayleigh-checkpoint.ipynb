{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from scipy import special\n",
    "from tensorflow.keras import layers\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 16\n",
    "k = int(np.log2(M))\n",
    "n = 1\n",
    "TRAINING_SNR = 7\n",
    "rayleigh = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EbNo_to_noise(ebnodb):\n",
    "    '''Transform EbNo[dB]/snr to noise power'''\n",
    "    ebno = 10**(ebnodb/10)\n",
    "    noise_std = 1/np.sqrt(2*(k/n)*ebno) \n",
    "    return noise_std\n",
    "\n",
    "def SNR_to_noise(snrdb):\n",
    "    '''Transform EbNo[dB]/snr to noise power'''\n",
    "    snr = 10**(snrdb/10)\n",
    "    noise_std = 1/np.sqrt(2*snr)\n",
    "    return noise_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://colab.research.google.com/github/google-research/google-research/blob/master/vbmi/vbmi_demo.ipynb#scrollTo=LwSIGeXlD11E\n",
    "# concatenated critic\n",
    "\n",
    "class NN_function(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim, layers, activation, **extra_kwargs):\n",
    "        super(NN_function, self).__init__()\n",
    "        self._f = tf.keras.Sequential(\n",
    "          [tf.keras.layers.Dense(hidden_dim, activation) for _ in range(layers)] +\n",
    "          [tf.keras.layers.Dense(1)])\n",
    "\n",
    "    def call(self, x, y):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x_tiled = tf.tile(x[None, :],  (batch_size, 1, 1))\n",
    "        y_tiled = tf.tile(y[:, None],  (1, batch_size, 1))\n",
    "        xy_pairs = tf.reshape(tf.concat((x_tiled, y_tiled), axis=2), [batch_size * batch_size, -1])\n",
    "        scores = self._f(xy_pairs)\n",
    "        return tf.transpose(tf.reshape(scores, [batch_size, batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_params = {\n",
    "    'layers': 2,\n",
    "    'hidden_dim': 256,\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "def MINE(scores):  \n",
    "    def marg(x):\n",
    "        batch_size = x.shape[0]\n",
    "        marg_ = tf.reduce_mean(tf.exp(x - tf.linalg.tensor_diag(np.inf * tf.ones(batch_size))))\n",
    "        return marg_*((batch_size*batch_size)/(batch_size*(batch_size-1.)))\n",
    "    joint_term = tf.reduce_mean(tf.linalg.diag_part(scores))\n",
    "    marg_term = marg(scores)\n",
    "    return joint_term - tf.math.log(marg_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = EbNo_to_noise(TRAINING_SNR)\n",
    "# custom functions / layers without weights\n",
    "norm_layer = keras.layers.Lambda(lambda x: tf.divide(x,tf.sqrt(2*tf.reduce_mean(tf.square(x)))))\n",
    "shape_layer = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,2,n]))\n",
    "shape_layer2 = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,2*n]))\n",
    "channel_layer = keras.layers.Lambda(lambda x: \n",
    "                    x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std))\n",
    "def sample_Rayleigh_channel(x, noise_std):\n",
    "    h_sample = (1/np.sqrt(2))*tf.sqrt(tf.random.normal(tf.shape(x))**2+tf.random.normal(tf.shape(x))**2)\n",
    "    z_sample = tf.random.normal(tf.shape(x), stddev = noise_std)\n",
    "    y_sample = x + tf.divide(z_sample,h_sample)\n",
    "    return tf.cast(y_sample, tf.float32)\n",
    "\n",
    "rayleigh_channel_layer = keras.layers.Lambda(lambda x: sample_Rayleigh_channel(x,noise_std))\n",
    "\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "            keras.layers.Embedding(M, M, embeddings_initializer='glorot_normal', input_length=1),\n",
    "            keras.layers.Dense(M, activation=\"elu\"),\n",
    "            keras.layers.Dense(2*n, activation=None),\n",
    "            shape_layer,\n",
    "            norm_layer])\n",
    "if rayleigh:\n",
    "    channel = keras.models.Sequential([rayleigh_channel_layer])\n",
    "else:\n",
    "    channel = keras.models.Sequential([channel_layer])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "                keras.layers.InputLayer(input_shape=[2,n]),\n",
    "                shape_layer2,\n",
    "                keras.layers.Dense(M, activation=\"elu\"),\n",
    "                keras.layers.Dense(M, activation=\"softmax\")\n",
    "                ])\n",
    "\n",
    "autoencoder = keras.models.Sequential([encoder, channel, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B_Ber_m(input_msg, msg):\n",
    "    '''Calculate the Batch Bit Error Rate'''\n",
    "    batch_size = input_msg.shape[0]\n",
    "    pred_error = tf.not_equal(tf.reshape(input_msg, shape=(-1,batch_size)), tf.argmax(msg, 1)) \n",
    "    bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))\n",
    "    return bber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(batch_size=32):\n",
    "    msg = np.random.randint(M, size=(batch_size,1))\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoding(M=16, n=1):\n",
    "    inp = np.arange(0,M)\n",
    "    coding = encoder.predict(inp)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    plt.plot(coding[:,0], coding[:, 1], \"b.\")\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(-2, 2)\n",
    "    plt.gca().set_xlim(-2, 2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_noisy_codeword(data):\n",
    "    rcvd_word = data[1:2000]\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    plt.plot(rcvd_word[:,0], rcvd_word[:, 1], \"b.\")\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(-2, 2)\n",
    "    plt.gca().set_xlim(-2, 2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "mean_loss = keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(step, epoch, mean_loss, X_batch, y_pred, plot_encoding):\n",
    "    template = 'Iteration: {}, Epoch: {}, Loss: {:.5f}, Batch_BER: {:.5f}'\n",
    "    if step % 10 == 0:\n",
    "        print(template.format(step, epoch, mean_loss.result(), B_Ber_m(X_batch, y_pred)))\n",
    "        if plot_encoding:\n",
    "            test_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_loss(epoch, mean_loss, X_batch, y_pred):\n",
    "        template_outer_loop = 'Interim result for Epoch: {}, Loss: {:.5f}, Batch_BER: {:.5f}'\n",
    "        print(template_outer_loop.format(epoch, mean_loss.result(), B_Ber_m(X_batch, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(x_sample, y_sample):\n",
    "    x_shaped = tf.reshape(x_sample, shape=[-1,2*n])\n",
    "    y_shaped = tf.reshape(y_sample, shape=[-1,2*n])\n",
    "    x_sample1, x_sample2 = tf.split(x_shaped, num_or_size_splits=2)\n",
    "    y_sample1, y_sample2 = tf.split(y_shaped, num_or_size_splits=2)\n",
    "    joint_sample = tf.concat([x_sample1, y_sample1], axis=1)\n",
    "    marg_sample = tf.concat([x_sample2, y_sample1], axis=1)\n",
    "    return joint_sample, marg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mi(NN_estimation, n_epochs=5, n_steps=20, batch_size=200, learning_rate=0.005):\n",
    "    optimizer_mi = keras.optimizers.Nadam(lr=learning_rate)\n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        print(\"Training in Epoch {}/{}\".format(epoch, n_epochs)) \n",
    "        for step in range(1, n_steps + 1):\n",
    "            X_batch = random_sample(batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_enc = encoder(X_batch, training=True)\n",
    "                y_recv = tf.grad_pass_through(channel)(x_enc)\n",
    "                x = tf.reshape(x_enc, shape=[batch_size,2*n])\n",
    "                y = tf.reshape(y_recv, shape=[batch_size,2*n])\n",
    "                score = NN_estimation(x,y)\n",
    "                loss = -MINE(score)\n",
    "                gradients = tape.gradient(loss, NN_estimation.trainable_variables) \n",
    "                optimizer_mi.apply_gradients(zip(gradients, NN_estimation.trainable_variables))\n",
    "            mi_avg = -mean_loss(loss)\n",
    "        print('Epoch: {}, Mi is {}'.format(epoch, mi_avg))\n",
    "        mean_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(n_epochs=5, n_steps=20, batch_size=200, learning_rate=0.005, plot_encoding=True):\n",
    "    optimizer_ae = keras.optimizers.Nadam(lr=learning_rate)\n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        print(\"Training Bob in Epoch {}/{}\".format(epoch, n_epochs)) \n",
    "        for step in range(1, n_steps + 1):\n",
    "            X_batch  = random_sample(batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = autoencoder(X_batch, training=True)\n",
    "                loss = tf.reduce_mean(loss_fn(X_batch, y_pred))\n",
    "                gradients = tape.gradient(loss, decoder.trainable_variables) \n",
    "                optimizer_ae.apply_gradients(zip(gradients, decoder.trainable_variables)) \n",
    "            mean_loss(loss)\n",
    "            plot_loss(step, epoch, mean_loss, X_batch, y_pred, plot_encoding)\n",
    "        plot_batch_loss(epoch, mean_loss, X_batch, y_pred) \n",
    "        mean_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(NN_estimation, n_epochs=5, n_steps=20, batch_size=200, learning_rate=0.05):\n",
    "    optimizer_mi = keras.optimizers.Nadam(lr=0.005)\n",
    "    optimizer_ae = keras.optimizers.Nadam(lr=learning_rate)\n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        print(\"Training Bob in Epoch {}/{}\".format(epoch, n_epochs)) \n",
    "        for step in range(1, n_steps + 1):\n",
    "            X_batch  = random_sample(batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                x_enc = encoder(X_batch, training=True)\n",
    "                y_recv = tf.grad_pass_through(channel)(x_enc) #forward pass:channel; backward pass Identity\n",
    "                x = tf.reshape(x_enc, shape=[batch_size,2*n])\n",
    "                y = tf.reshape(y_recv, shape=[batch_size,2*n])\n",
    "                score = NN_estimation(x,y)\n",
    "                loss = -MINE(score)\n",
    "                gradients = tape.gradient(loss, encoder.trainable_variables) \n",
    "                optimizer_ae.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
    "            mi_avg = -mean_loss(loss)\n",
    "        with tf.GradientTape() as tape:\n",
    "            X_batch  = random_sample(batch_size) \n",
    "            x_enc = encoder(X_batch, training=True)\n",
    "            y_recv = tf.grad_pass_through(channel)(x_enc)\n",
    "            x = tf.reshape(x_enc, shape=[batch_size,2*n])\n",
    "            y = tf.reshape(y_recv, shape=[batch_size,2*n])\n",
    "            score = NN_estimation(x,y)\n",
    "            loss = -MINE(score)\n",
    "            gradients = tape.gradient(loss, NN_estimation.trainable_variables) \n",
    "            optimizer_mi.apply_gradients(zip(gradients, NN_estimation.trainable_variables))\n",
    "        print('Epoch: {}, Mi is {}'.format(epoch, mi_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_AE():\n",
    "    '''Calculate Bit Error for varying SNRs'''\n",
    "    snr_range = np.linspace(0, 15, 31)\n",
    "    bber_vec = [None] * len(snr_range)\n",
    "        \n",
    "    for db in range(len(snr_range)):\n",
    "        for it in range(1,1000):\n",
    "            noise_std = EbNo_to_noise(snr_range[db])\n",
    "            X_batch  = random_sample(500)\n",
    "            code_word = encoder(X_batch)\n",
    "            if rayleigh:\n",
    "                rcvd_word = sample_Rayleigh_channel(code_word, noise_std)\n",
    "            else:\n",
    "                rcvd_word = code_word + tf.random.normal(tf.shape(code_word), mean=0.0, stddev=noise_std)\n",
    "            dcoded_msg = decoder(rcvd_word)\n",
    "            bber = B_Ber_m(X_batch, dcoded_msg)\n",
    "            bber_avg = mean_loss(bber)\n",
    "        bber_vec[db] = bber_avg\n",
    "        mean_loss.reset_states()\n",
    "        if (db % 6 == 0) & (db > 0):\n",
    "            print(f'Progress: {db} of {30} parts')\n",
    "\n",
    "    return (snr_range, bber_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in Epoch 1/1\n",
      "Epoch: 1, Mi is 1.492371678352356\n",
      "Training Bob in Epoch 1/5\n",
      "Epoch: 1, Mi is 1.5568511486053467\n",
      "Training Bob in Epoch 2/5\n",
      "Epoch: 2, Mi is 1.602770209312439\n",
      "Training Bob in Epoch 3/5\n",
      "Epoch: 3, Mi is 1.652502179145813\n",
      "Training Bob in Epoch 4/5\n",
      "Epoch: 4, Mi is 1.6629245281219482\n",
      "Training Bob in Epoch 5/5\n",
      "Epoch: 5, Mi is 1.6791517734527588\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bob in Epoch 1/5\n",
      "Iteration: 10, Epoch: 1, Loss: -1.65714, Batch_BER: 0.87600\n",
      "Iteration: 20, Epoch: 1, Loss: -1.63611, Batch_BER: 0.74200\n",
      "Iteration: 30, Epoch: 1, Loss: -1.61605, Batch_BER: 0.71600\n",
      "Iteration: 40, Epoch: 1, Loss: -1.59700, Batch_BER: 0.69600\n",
      "Iteration: 50, Epoch: 1, Loss: -1.57886, Batch_BER: 0.58200\n",
      "Iteration: 60, Epoch: 1, Loss: -1.56161, Batch_BER: 0.57400\n",
      "Iteration: 70, Epoch: 1, Loss: -1.54512, Batch_BER: 0.50600\n",
      "Iteration: 80, Epoch: 1, Loss: -1.52916, Batch_BER: 0.51400\n",
      "Iteration: 90, Epoch: 1, Loss: -1.51376, Batch_BER: 0.46200\n",
      "Iteration: 100, Epoch: 1, Loss: -1.49888, Batch_BER: 0.44800\n",
      "Iteration: 110, Epoch: 1, Loss: -1.48439, Batch_BER: 0.44000\n",
      "Iteration: 120, Epoch: 1, Loss: -1.47028, Batch_BER: 0.35200\n",
      "Iteration: 130, Epoch: 1, Loss: -1.45654, Batch_BER: 0.35400\n",
      "Iteration: 140, Epoch: 1, Loss: -1.44305, Batch_BER: 0.35000\n",
      "Iteration: 150, Epoch: 1, Loss: -1.42987, Batch_BER: 0.29400\n",
      "Iteration: 160, Epoch: 1, Loss: -1.41705, Batch_BER: 0.27800\n",
      "Iteration: 170, Epoch: 1, Loss: -1.40444, Batch_BER: 0.27400\n",
      "Iteration: 180, Epoch: 1, Loss: -1.39206, Batch_BER: 0.29800\n",
      "Iteration: 190, Epoch: 1, Loss: -1.37971, Batch_BER: 0.29800\n",
      "Iteration: 200, Epoch: 1, Loss: -1.36757, Batch_BER: 0.26200\n",
      "Iteration: 210, Epoch: 1, Loss: -1.35559, Batch_BER: 0.28200\n",
      "Iteration: 220, Epoch: 1, Loss: -1.34381, Batch_BER: 0.24800\n",
      "Iteration: 230, Epoch: 1, Loss: -1.33236, Batch_BER: 0.21800\n",
      "Iteration: 240, Epoch: 1, Loss: -1.32118, Batch_BER: 0.27200\n",
      "Iteration: 250, Epoch: 1, Loss: -1.30975, Batch_BER: 0.29800\n",
      "Iteration: 260, Epoch: 1, Loss: -1.29853, Batch_BER: 0.26800\n",
      "Iteration: 270, Epoch: 1, Loss: -1.28755, Batch_BER: 0.25600\n",
      "Iteration: 280, Epoch: 1, Loss: -1.27659, Batch_BER: 0.23800\n",
      "Iteration: 290, Epoch: 1, Loss: -1.26584, Batch_BER: 0.27400\n",
      "Iteration: 300, Epoch: 1, Loss: -1.25523, Batch_BER: 0.27600\n",
      "Iteration: 310, Epoch: 1, Loss: -1.24465, Batch_BER: 0.25800\n",
      "Iteration: 320, Epoch: 1, Loss: -1.23403, Batch_BER: 0.26600\n",
      "Iteration: 330, Epoch: 1, Loss: -1.22379, Batch_BER: 0.26200\n",
      "Iteration: 340, Epoch: 1, Loss: -1.21348, Batch_BER: 0.26800\n",
      "Iteration: 350, Epoch: 1, Loss: -1.20323, Batch_BER: 0.26400\n",
      "Iteration: 360, Epoch: 1, Loss: -1.19319, Batch_BER: 0.26600\n",
      "Iteration: 370, Epoch: 1, Loss: -1.18337, Batch_BER: 0.25400\n",
      "Iteration: 380, Epoch: 1, Loss: -1.17355, Batch_BER: 0.24600\n",
      "Iteration: 390, Epoch: 1, Loss: -1.16378, Batch_BER: 0.28200\n",
      "Iteration: 400, Epoch: 1, Loss: -1.15395, Batch_BER: 0.24000\n",
      "Interim result for Epoch: 1, Loss: -1.15395, Batch_BER: 0.24000\n",
      "Training Bob in Epoch 2/5\n",
      "Iteration: 10, Epoch: 2, Loss: 1.14930, Batch_BER: 0.25800\n",
      "Iteration: 20, Epoch: 2, Loss: 1.13347, Batch_BER: 0.24000\n",
      "Iteration: 30, Epoch: 2, Loss: 1.14584, Batch_BER: 0.24200\n",
      "Iteration: 40, Epoch: 2, Loss: 1.16014, Batch_BER: 0.31000\n",
      "Iteration: 50, Epoch: 2, Loss: 1.15145, Batch_BER: 0.22400\n",
      "Iteration: 60, Epoch: 2, Loss: 1.14673, Batch_BER: 0.24200\n",
      "Iteration: 70, Epoch: 2, Loss: 1.14519, Batch_BER: 0.23000\n",
      "Iteration: 80, Epoch: 2, Loss: 1.14594, Batch_BER: 0.22400\n",
      "Iteration: 90, Epoch: 2, Loss: 1.14592, Batch_BER: 0.26400\n",
      "Iteration: 100, Epoch: 2, Loss: 1.14458, Batch_BER: 0.26200\n",
      "Iteration: 110, Epoch: 2, Loss: 1.14764, Batch_BER: 0.23600\n",
      "Iteration: 120, Epoch: 2, Loss: 1.14567, Batch_BER: 0.23600\n",
      "Iteration: 130, Epoch: 2, Loss: 1.14654, Batch_BER: 0.27200\n",
      "Iteration: 140, Epoch: 2, Loss: 1.14540, Batch_BER: 0.25600\n",
      "Iteration: 150, Epoch: 2, Loss: 1.14472, Batch_BER: 0.26000\n",
      "Iteration: 160, Epoch: 2, Loss: 1.14426, Batch_BER: 0.28600\n",
      "Iteration: 170, Epoch: 2, Loss: 1.14565, Batch_BER: 0.26600\n",
      "Iteration: 180, Epoch: 2, Loss: 1.14516, Batch_BER: 0.24600\n",
      "Iteration: 190, Epoch: 2, Loss: 1.14362, Batch_BER: 0.27000\n",
      "Iteration: 200, Epoch: 2, Loss: 1.14267, Batch_BER: 0.27600\n",
      "Iteration: 210, Epoch: 2, Loss: 1.14348, Batch_BER: 0.22000\n",
      "Iteration: 220, Epoch: 2, Loss: 1.14468, Batch_BER: 0.23400\n",
      "Iteration: 230, Epoch: 2, Loss: 1.14473, Batch_BER: 0.29000\n",
      "Iteration: 240, Epoch: 2, Loss: 1.14474, Batch_BER: 0.26600\n",
      "Iteration: 250, Epoch: 2, Loss: 1.14363, Batch_BER: 0.24200\n",
      "Iteration: 260, Epoch: 2, Loss: 1.14356, Batch_BER: 0.26600\n",
      "Iteration: 270, Epoch: 2, Loss: 1.14465, Batch_BER: 0.25400\n",
      "Iteration: 280, Epoch: 2, Loss: 1.14540, Batch_BER: 0.26400\n",
      "Iteration: 290, Epoch: 2, Loss: 1.14403, Batch_BER: 0.25800\n",
      "Iteration: 300, Epoch: 2, Loss: 1.14419, Batch_BER: 0.24600\n",
      "Iteration: 310, Epoch: 2, Loss: 1.14505, Batch_BER: 0.24600\n",
      "Iteration: 320, Epoch: 2, Loss: 1.14638, Batch_BER: 0.26600\n",
      "Iteration: 330, Epoch: 2, Loss: 1.14583, Batch_BER: 0.27200\n",
      "Iteration: 340, Epoch: 2, Loss: 1.14751, Batch_BER: 0.25800\n",
      "Iteration: 350, Epoch: 2, Loss: 1.14847, Batch_BER: 0.25200\n",
      "Iteration: 360, Epoch: 2, Loss: 1.14842, Batch_BER: 0.27800\n",
      "Iteration: 370, Epoch: 2, Loss: 1.14763, Batch_BER: 0.24600\n",
      "Iteration: 380, Epoch: 2, Loss: 1.14679, Batch_BER: 0.25800\n",
      "Iteration: 390, Epoch: 2, Loss: 1.14730, Batch_BER: 0.24800\n",
      "Iteration: 400, Epoch: 2, Loss: 1.14595, Batch_BER: 0.23200\n",
      "Interim result for Epoch: 2, Loss: 1.14595, Batch_BER: 0.23200\n",
      "Training Bob in Epoch 3/5\n",
      "Iteration: 10, Epoch: 3, Loss: 1.16644, Batch_BER: 0.28600\n",
      "Iteration: 20, Epoch: 3, Loss: 1.14904, Batch_BER: 0.28000\n",
      "Iteration: 30, Epoch: 3, Loss: 1.14330, Batch_BER: 0.27400\n",
      "Iteration: 40, Epoch: 3, Loss: 1.15322, Batch_BER: 0.28000\n",
      "Iteration: 50, Epoch: 3, Loss: 1.14230, Batch_BER: 0.25200\n",
      "Iteration: 60, Epoch: 3, Loss: 1.14168, Batch_BER: 0.29800\n",
      "Iteration: 70, Epoch: 3, Loss: 1.14046, Batch_BER: 0.23800\n",
      "Iteration: 80, Epoch: 3, Loss: 1.13730, Batch_BER: 0.25200\n",
      "Iteration: 90, Epoch: 3, Loss: 1.13722, Batch_BER: 0.25600\n",
      "Iteration: 100, Epoch: 3, Loss: 1.13182, Batch_BER: 0.27200\n",
      "Iteration: 110, Epoch: 3, Loss: 1.13273, Batch_BER: 0.22000\n",
      "Iteration: 120, Epoch: 3, Loss: 1.12953, Batch_BER: 0.27000\n",
      "Iteration: 130, Epoch: 3, Loss: 1.12930, Batch_BER: 0.24600\n",
      "Iteration: 140, Epoch: 3, Loss: 1.13118, Batch_BER: 0.26400\n",
      "Iteration: 150, Epoch: 3, Loss: 1.13133, Batch_BER: 0.23600\n",
      "Iteration: 160, Epoch: 3, Loss: 1.13145, Batch_BER: 0.27200\n",
      "Iteration: 170, Epoch: 3, Loss: 1.13071, Batch_BER: 0.25600\n",
      "Iteration: 180, Epoch: 3, Loss: 1.12981, Batch_BER: 0.23800\n",
      "Iteration: 190, Epoch: 3, Loss: 1.12762, Batch_BER: 0.25800\n",
      "Iteration: 200, Epoch: 3, Loss: 1.12538, Batch_BER: 0.24000\n",
      "Iteration: 210, Epoch: 3, Loss: 1.12469, Batch_BER: 0.25200\n",
      "Iteration: 220, Epoch: 3, Loss: 1.12234, Batch_BER: 0.25200\n",
      "Iteration: 230, Epoch: 3, Loss: 1.12307, Batch_BER: 0.26000\n",
      "Iteration: 240, Epoch: 3, Loss: 1.12244, Batch_BER: 0.22800\n",
      "Iteration: 250, Epoch: 3, Loss: 1.12494, Batch_BER: 0.28600\n",
      "Iteration: 260, Epoch: 3, Loss: 1.12403, Batch_BER: 0.25400\n",
      "Iteration: 270, Epoch: 3, Loss: 1.12354, Batch_BER: 0.24800\n",
      "Iteration: 280, Epoch: 3, Loss: 1.12329, Batch_BER: 0.29000\n",
      "Iteration: 290, Epoch: 3, Loss: 1.12414, Batch_BER: 0.24000\n",
      "Iteration: 300, Epoch: 3, Loss: 1.12402, Batch_BER: 0.24800\n",
      "Iteration: 310, Epoch: 3, Loss: 1.12445, Batch_BER: 0.24400\n",
      "Iteration: 320, Epoch: 3, Loss: 1.12494, Batch_BER: 0.25800\n",
      "Iteration: 330, Epoch: 3, Loss: 1.12408, Batch_BER: 0.23000\n",
      "Iteration: 340, Epoch: 3, Loss: 1.12388, Batch_BER: 0.25200\n",
      "Iteration: 350, Epoch: 3, Loss: 1.12240, Batch_BER: 0.23600\n",
      "Iteration: 360, Epoch: 3, Loss: 1.12315, Batch_BER: 0.27000\n",
      "Iteration: 370, Epoch: 3, Loss: 1.12267, Batch_BER: 0.26600\n",
      "Iteration: 380, Epoch: 3, Loss: 1.12219, Batch_BER: 0.25400\n",
      "Iteration: 390, Epoch: 3, Loss: 1.12280, Batch_BER: 0.26400\n",
      "Iteration: 400, Epoch: 3, Loss: 1.12206, Batch_BER: 0.24800\n",
      "Interim result for Epoch: 3, Loss: 1.12206, Batch_BER: 0.24800\n",
      "Training Bob in Epoch 4/5\n",
      "Iteration: 10, Epoch: 4, Loss: 1.10936, Batch_BER: 0.23200\n",
      "Iteration: 20, Epoch: 4, Loss: 1.12652, Batch_BER: 0.25600\n",
      "Iteration: 30, Epoch: 4, Loss: 1.13850, Batch_BER: 0.26600\n",
      "Iteration: 40, Epoch: 4, Loss: 1.13872, Batch_BER: 0.27000\n",
      "Iteration: 50, Epoch: 4, Loss: 1.13887, Batch_BER: 0.26600\n",
      "Iteration: 60, Epoch: 4, Loss: 1.13388, Batch_BER: 0.26400\n",
      "Iteration: 70, Epoch: 4, Loss: 1.13076, Batch_BER: 0.25400\n",
      "Iteration: 80, Epoch: 4, Loss: 1.13068, Batch_BER: 0.25200\n",
      "Iteration: 90, Epoch: 4, Loss: 1.13085, Batch_BER: 0.22000\n",
      "Iteration: 100, Epoch: 4, Loss: 1.13085, Batch_BER: 0.23800\n",
      "Iteration: 110, Epoch: 4, Loss: 1.13267, Batch_BER: 0.23600\n",
      "Iteration: 120, Epoch: 4, Loss: 1.12738, Batch_BER: 0.24200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 130, Epoch: 4, Loss: 1.12973, Batch_BER: 0.27600\n",
      "Iteration: 140, Epoch: 4, Loss: 1.12993, Batch_BER: 0.24800\n",
      "Iteration: 150, Epoch: 4, Loss: 1.13071, Batch_BER: 0.26600\n",
      "Iteration: 160, Epoch: 4, Loss: 1.12968, Batch_BER: 0.29200\n",
      "Iteration: 170, Epoch: 4, Loss: 1.12850, Batch_BER: 0.24400\n",
      "Iteration: 180, Epoch: 4, Loss: 1.12649, Batch_BER: 0.26200\n",
      "Iteration: 190, Epoch: 4, Loss: 1.12542, Batch_BER: 0.25400\n",
      "Iteration: 200, Epoch: 4, Loss: 1.12466, Batch_BER: 0.24200\n",
      "Iteration: 210, Epoch: 4, Loss: 1.12782, Batch_BER: 0.24400\n",
      "Iteration: 220, Epoch: 4, Loss: 1.12695, Batch_BER: 0.21600\n",
      "Iteration: 230, Epoch: 4, Loss: 1.12563, Batch_BER: 0.25400\n",
      "Iteration: 240, Epoch: 4, Loss: 1.12611, Batch_BER: 0.24800\n",
      "Iteration: 250, Epoch: 4, Loss: 1.12603, Batch_BER: 0.22800\n",
      "Iteration: 260, Epoch: 4, Loss: 1.12426, Batch_BER: 0.24800\n",
      "Iteration: 270, Epoch: 4, Loss: 1.12552, Batch_BER: 0.24800\n",
      "Iteration: 280, Epoch: 4, Loss: 1.12667, Batch_BER: 0.25800\n",
      "Iteration: 290, Epoch: 4, Loss: 1.12524, Batch_BER: 0.26000\n",
      "Iteration: 300, Epoch: 4, Loss: 1.12413, Batch_BER: 0.29600\n",
      "Iteration: 310, Epoch: 4, Loss: 1.12518, Batch_BER: 0.23800\n",
      "Iteration: 320, Epoch: 4, Loss: 1.12489, Batch_BER: 0.25400\n",
      "Iteration: 330, Epoch: 4, Loss: 1.12410, Batch_BER: 0.26400\n",
      "Iteration: 340, Epoch: 4, Loss: 1.12450, Batch_BER: 0.24000\n",
      "Iteration: 350, Epoch: 4, Loss: 1.12541, Batch_BER: 0.29600\n",
      "Iteration: 360, Epoch: 4, Loss: 1.12598, Batch_BER: 0.26800\n",
      "Iteration: 370, Epoch: 4, Loss: 1.12380, Batch_BER: 0.23000\n",
      "Iteration: 380, Epoch: 4, Loss: 1.12343, Batch_BER: 0.26000\n",
      "Iteration: 390, Epoch: 4, Loss: 1.12295, Batch_BER: 0.24600\n",
      "Iteration: 400, Epoch: 4, Loss: 1.12179, Batch_BER: 0.24800\n",
      "Interim result for Epoch: 4, Loss: 1.12179, Batch_BER: 0.24800\n",
      "Training Bob in Epoch 5/5\n",
      "Iteration: 10, Epoch: 5, Loss: 1.12406, Batch_BER: 0.22800\n",
      "Iteration: 20, Epoch: 5, Loss: 1.09145, Batch_BER: 0.25200\n",
      "Iteration: 30, Epoch: 5, Loss: 1.08990, Batch_BER: 0.22800\n",
      "Iteration: 40, Epoch: 5, Loss: 1.08844, Batch_BER: 0.24600\n",
      "Iteration: 50, Epoch: 5, Loss: 1.09978, Batch_BER: 0.29400\n",
      "Iteration: 60, Epoch: 5, Loss: 1.10904, Batch_BER: 0.27800\n",
      "Iteration: 70, Epoch: 5, Loss: 1.10657, Batch_BER: 0.23000\n",
      "Iteration: 80, Epoch: 5, Loss: 1.10932, Batch_BER: 0.24800\n",
      "Iteration: 90, Epoch: 5, Loss: 1.11353, Batch_BER: 0.27800\n",
      "Iteration: 100, Epoch: 5, Loss: 1.11218, Batch_BER: 0.26200\n",
      "Iteration: 110, Epoch: 5, Loss: 1.11109, Batch_BER: 0.28800\n",
      "Iteration: 120, Epoch: 5, Loss: 1.11358, Batch_BER: 0.26200\n",
      "Iteration: 130, Epoch: 5, Loss: 1.11135, Batch_BER: 0.23000\n",
      "Iteration: 140, Epoch: 5, Loss: 1.11286, Batch_BER: 0.26000\n",
      "Iteration: 150, Epoch: 5, Loss: 1.11367, Batch_BER: 0.24400\n",
      "Iteration: 160, Epoch: 5, Loss: 1.11314, Batch_BER: 0.20800\n",
      "Iteration: 170, Epoch: 5, Loss: 1.11403, Batch_BER: 0.26400\n",
      "Iteration: 180, Epoch: 5, Loss: 1.11287, Batch_BER: 0.28000\n",
      "Iteration: 190, Epoch: 5, Loss: 1.11171, Batch_BER: 0.26200\n",
      "Iteration: 200, Epoch: 5, Loss: 1.11092, Batch_BER: 0.26000\n",
      "Iteration: 210, Epoch: 5, Loss: 1.11203, Batch_BER: 0.25000\n",
      "Iteration: 220, Epoch: 5, Loss: 1.11215, Batch_BER: 0.27200\n",
      "Iteration: 230, Epoch: 5, Loss: 1.11322, Batch_BER: 0.22600\n",
      "Iteration: 240, Epoch: 5, Loss: 1.11241, Batch_BER: 0.21800\n",
      "Iteration: 250, Epoch: 5, Loss: 1.11434, Batch_BER: 0.26000\n",
      "Iteration: 260, Epoch: 5, Loss: 1.11489, Batch_BER: 0.29200\n",
      "Iteration: 270, Epoch: 5, Loss: 1.11504, Batch_BER: 0.24400\n",
      "Iteration: 280, Epoch: 5, Loss: 1.11520, Batch_BER: 0.27200\n",
      "Iteration: 290, Epoch: 5, Loss: 1.11558, Batch_BER: 0.23000\n",
      "Iteration: 300, Epoch: 5, Loss: 1.11557, Batch_BER: 0.25400\n",
      "Iteration: 310, Epoch: 5, Loss: 1.11493, Batch_BER: 0.24600\n",
      "Iteration: 320, Epoch: 5, Loss: 1.11354, Batch_BER: 0.26400\n",
      "Iteration: 330, Epoch: 5, Loss: 1.11332, Batch_BER: 0.25200\n",
      "Iteration: 340, Epoch: 5, Loss: 1.11253, Batch_BER: 0.26400\n",
      "Iteration: 350, Epoch: 5, Loss: 1.11179, Batch_BER: 0.26000\n",
      "Iteration: 360, Epoch: 5, Loss: 1.11044, Batch_BER: 0.26800\n",
      "Iteration: 370, Epoch: 5, Loss: 1.11051, Batch_BER: 0.24800\n",
      "Iteration: 380, Epoch: 5, Loss: 1.10916, Batch_BER: 0.24200\n",
      "Iteration: 390, Epoch: 5, Loss: 1.10744, Batch_BER: 0.21800\n",
      "Iteration: 400, Epoch: 5, Loss: 1.10849, Batch_BER: 0.26000\n",
      "Interim result for Epoch: 5, Loss: 1.10849, Batch_BER: 0.26000\n"
     ]
    }
   ],
   "source": [
    "score_fn = NN_function(**critic_params)\n",
    "train_mi(score_fn, n_epochs=1, n_steps=500, batch_size=64)\n",
    "train_encoder(score_fn, n_epochs=5, n_steps=400, batch_size=64, learning_rate=0.005)\n",
    "test_encoding(M, 1)\n",
    "train_decoder(n_epochs=5, n_steps=400, batch_size=500, learning_rate=0.005, plot_encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 6 of 30 parts\n",
      "Progress: 12 of 30 parts\n",
      "Progress: 18 of 30 parts\n",
      "Progress: 24 of 30 parts\n"
     ]
    }
   ],
   "source": [
    "bber_data = Test_AE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate 16 QAM Error\n",
    "def SIXT_QAM_sim(ebno):\n",
    "    return (3.0/2)*special.erfc(np.sqrt((4.0/10)*10.**(ebno/10)))\n",
    "\n",
    "def MQAM_rayleigh_approx(M, ebnodb):\n",
    "    ebno = 10.**(ebnodb/10)\n",
    "    esno = 4*ebno\n",
    "    #Goldsmith, p.185, 6.3.2, Eqn 6.61, alphaM=4, betaM=3/(M-1)\n",
    "    a=3.25 #adjusted mean number of neighbors\n",
    "    b=3/(M-1)\n",
    "    e=b*esno\n",
    "    return (a/2)*(1-np.sqrt(0.5*e / (1+0.5*e) ) ), a/(2*b*esno)\n",
    "\n",
    "ebnodbs = np.linspace(0,15,16)\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "plt.semilogy(bber_data[0], bber_data[1], 'o-')\n",
    "\n",
    "a, b = MQAM_rayleigh_approx(16,ebnodbs)\n",
    "plt.plot(ebnodbs, a);\n",
    "#plt.semilogy(ebnodbs, MQAM_rayleigh_approx(M, ebnodbs), '^-');\n",
    "plt.gca().set_ylim(1e-2, 1)\n",
    "plt.gca().set_xlim(0, 15)\n",
    "plt.ylabel(\"Batch Symbol Error Rate\", fontsize=14, rotation=90)\n",
    "plt.xlabel(\"EbN0 [dB]\", fontsize=18)\n",
    "plt.legend(['AE with MINE', '16QAM'],\n",
    "           prop={'size': 14}, loc='upper right');\n",
    "plt.grid(True, which=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
